#!/usr/bin/env python3
"""
æµ‹è¯•æœ¬åœ°æ¨¡å‹åŠ è½½çš„è„šæœ¬
"""

import os
import sys
import torch
from pathlib import Path

def test_qwen2vl():
    """æµ‹è¯•Qwen2VLæ¨¡å‹"""
    print("=== æµ‹è¯• Qwen2VL æ¨¡å‹ ===")
    
    try:
        from models.qwen import Qwen2VL
        
        model_path = "models/Qwen2-VL-7B-Instruct"
        if not os.path.exists(model_path):
            print(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
            return False
            
        # åˆ›å»ºé…ç½®å¯¹è±¡
        class Config:
            def __init__(self, model_id):
                self.model_id = model_id
                self.max_new_tokens = 512
        
        config = Config(model_path)
        print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_path}")
        model = Qwen2VL(config)
        print("âœ… Qwen2VL æ¨¡å‹åŠ è½½æˆåŠŸ")
        return True
        
    except Exception as e:
        print(f"âŒ Qwen2VL æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return False

def test_opt27b():
    """æµ‹è¯•OPT-2.7Bæ¨¡å‹"""
    print("\n=== æµ‹è¯• OPT-2.7B æ¨¡å‹ ===")
    
    try:
        from models.llama import Llama3
        
        model_path = "models/opt-2.7b"
        if not os.path.exists(model_path):
            print(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
            return False
            
        # åˆ›å»ºé…ç½®å¯¹è±¡
        class Config:
            def __init__(self, model_id):
                self.model_id = model_id
                self.max_new_tokens = 512
        
        config = Config(model_path)
        print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_path}")
        model = Llama3(config)
        print("âœ… OPT-2.7B æ¨¡å‹åŠ è½½æˆåŠŸ")
        return True
        
    except Exception as e:
        print(f"âŒ OPT-2.7B æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return False

def test_config_files():
    """æµ‹è¯•é…ç½®æ–‡ä»¶"""
    print("\n=== æµ‹è¯•é…ç½®æ–‡ä»¶ ===")
    
    # æµ‹è¯•qwen2vlé…ç½®
    try:
        import yaml
        with open("config/model/qwen2vl.yaml", "r") as f:
            qwen_config = yaml.safe_load(f)
        print(f"âœ… Qwen2VL é…ç½®: {qwen_config.get('model_id')}")
    except Exception as e:
        print(f"âŒ Qwen2VL é…ç½®é”™è¯¯: {e}")
    
    # æµ‹è¯•llama31é…ç½®
    try:
        with open("config/model/llama31.yaml", "r") as f:
            llama_config = yaml.safe_load(f)
        print(f"âœ… Llama31 é…ç½®: {llama_config.get('model_id')}")
    except Exception as e:
        print(f"âŒ Llama31 é…ç½®é”™è¯¯: {e}")

def check_model_files():
    """æ£€æŸ¥æ¨¡å‹æ–‡ä»¶"""
    print("\n=== æ£€æŸ¥æ¨¡å‹æ–‡ä»¶ ===")
    
    models_to_check = {
        "Qwen2VL": "models/Qwen2-VL-7B-Instruct",
        "OPT-2.7B": "models/opt-2.7b"
    }
    
    for name, path in models_to_check.items():
        if os.path.exists(path):
            print(f"âœ… {name}: {path} (å­˜åœ¨)")
            
            # æ£€æŸ¥å…³é”®æ–‡ä»¶
            if name == "OPT-2.7B":
                # OPTæ¨¡å‹ä½¿ç”¨ä¸åŒçš„tokenizeræ–‡ä»¶
                key_files = ["config.json", "vocab.json", "tokenizer_config.json"]
            else:
                key_files = ["config.json", "tokenizer.json"]
                
            for file in key_files:
                file_path = os.path.join(path, file)
                if os.path.exists(file_path):
                    print(f"  âœ… {file}")
                else:
                    print(f"  âŒ {file} (ç¼ºå¤±)")
        else:
            print(f"âŒ {name}: {path} (ä¸å­˜åœ¨)")

if __name__ == "__main__":
    print("=== æœ¬åœ°æ¨¡å‹æµ‹è¯•å·¥å…· ===\n")
    
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
    check_model_files()
    
    # æµ‹è¯•é…ç½®æ–‡ä»¶
    test_config_files()
    
    # æµ‹è¯•æ¨¡å‹åŠ è½½
    qwen_success = test_qwen2vl()
    opt_success = test_opt27b()
    
    print("\n=== æµ‹è¯•ç»“æœ ===")
    if qwen_success and opt_success:
        print("ğŸ‰ æ‰€æœ‰æ¨¡å‹æµ‹è¯•é€šè¿‡ï¼å¯ä»¥å¼€å§‹æ¨ç†äº†")
        print("\nä¸‹ä¸€æ­¥:")
        print("python scripts/predict.py --config-name feta run-name=my_test_run")
    else:
        print("âŒ éƒ¨åˆ†æ¨¡å‹æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®") 